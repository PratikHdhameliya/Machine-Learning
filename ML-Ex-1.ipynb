{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1945026,"sourceType":"datasetVersion","datasetId":1160310}],"dockerImageVersionId":30474,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Machine Learning - Practical 1\n\nNames: Pratik Dhameliya,Injamam Ul Karim,Hasan Marwan Mahmood","metadata":{}},{"cell_type":"markdown","source":"This notebook provides you with the assignments and the overall code structure you need to complete the assignment. There are also questions that you need to answer in text form. Please use full sentences and reasonably correct spelling/grammar.\n\nRegarding submission & grading:\n\n- Work in groups of three and hand in your solution as a group.\n\n- Solutions need to be uploaded to StudIP until the submission date indicated in the course plan. Please upload a copy of this notebook and a PDF version of it after you ran it.\n\n- Solutions need to be presented to tutors in tutorial. Presentation dates are listed in the course plan. Every group member needs to be able to explain everything.\n\n- You have to solve N-1 practicals to get admission to the exam.\n\n- For plots you create yourself, all axes must be labeled. \n\n- Do not change the function interfaces.","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The  dataset","metadata":{}},{"cell_type":"markdown","source":"The dataset consists of over 20.000 materials and lists their physical features. From these features, we want to learn how to predict the critical temperature, i.e. the temperature we need to cool the material to so it becomes superconductive. First load and familiarize yourself with the data set a bit.","metadata":{}},{"cell_type":"code","source":"import os \n\ndata = pd.read_csv('/kaggle/input/superconductor-dataset/train.csv')\nprint(data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:55:59.261200Z","iopub.execute_input":"2023-05-05T11:55:59.261598Z","iopub.status.idle":"2023-05-05T11:55:59.308818Z","shell.execute_reply.started":"2023-05-05T11:55:59.261558Z","shell.execute_reply":"2023-05-05T11:55:59.307622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because the dataset is rather large, we prepare a small subset of the data as training set, and another subset as test set. To make the computations reproducible, we set the random seed.","metadata":{}},{"cell_type":"code","source":"target_clm = 'critical_temp'  # the critical temperature is our target variable\nn_trainset = 200  # size of the training set\nn_testset = 500  # size of the test set","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:55:59.310234Z","iopub.execute_input":"2023-05-05T11:55:59.310559Z","iopub.status.idle":"2023-05-05T11:55:59.316323Z","shell.execute_reply.started":"2023-05-05T11:55:59.310529Z","shell.execute_reply":"2023-05-05T11:55:59.314248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set random seed to make sure every test set is the same\nnp.random.seed(seed=1)\n\nidx = np.arange(data.shape[0])\nprint(idx)\nidx_shuffled = np.random.permutation(idx)  # shuffle indices to split into training and test set\nprint(idx_shuffled)\ntest_idx = idx_shuffled[:n_testset]\ntrain_idx = idx_shuffled[n_testset:n_testset+n_trainset]\ntrain_full_idx = idx_shuffled[n_testset:]\n\nX_test = data.loc[test_idx, data.columns != target_clm].values\ny_test = data.loc[test_idx, data.columns == target_clm].values\nprint('Test set shapes (X and y)', X_test.shape, y_test.shape)\n\nX_train = data.loc[train_idx, data.columns != target_clm].values\ny_train = data.loc[train_idx, data.columns == target_clm].values\nprint('Small training set shapes (X and y):', X_train.shape, y_train.shape)\n\nX_train_full = data.loc[train_full_idx, data.columns != target_clm].values\ny_train_full = data.loc[train_full_idx, data.columns == target_clm].values\nprint('Full training set shapes (X and y):', X_train_full.shape, y_train_full.shape)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:55:59.318487Z","iopub.execute_input":"2023-05-05T11:55:59.319439Z","iopub.status.idle":"2023-05-05T11:55:59.385640Z","shell.execute_reply.started":"2023-05-05T11:55:59.319402Z","shell.execute_reply":"2023-05-05T11:55:59.384500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 1: Plot the dataset\n\nTo explore the dataset, use `X_train_full` and `y_train_full` for two descriptive plots:\n\n* **Histogram** of the target variable. Use `plt.hist`.\n\n* **Scatterplots** relating the target variable to one of the feature values. For this you will need 81 scatterplots. Arrange them in one big figure with 9x9 subplots. Use `plt.scatter`. You may need to adjust the marker size and the alpha blending value. \n\nFurthermore, we need to normalize the data, such that each feature has a mean of zero mean and a variance of one. Implement a function `normalize` which normalizes the data. Print the means and standard variation of the first five features before and after. For simplicity we will normalize train and test set independently. It is better practice to normalize the test set with mean and variance of the test set. Maybe you can think of situations / reasons where this makes a difference.","metadata":{}},{"cell_type":"code","source":"# Histogram of the target variable\nplt.hist(y_train_full, color= 'skyblue', edgecolor = 'black')\nplt.title('Histogram of data')\nplt.xlabel('temp')\nplt.ylabel('frequency of each band')","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:55:59.387241Z","iopub.execute_input":"2023-05-05T11:55:59.387821Z","iopub.status.idle":"2023-05-05T11:55:59.716505Z","shell.execute_reply.started":"2023-05-05T11:55:59.387769Z","shell.execute_reply":"2023-05-05T11:55:59.715152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scatter plots of the target variable vs. features\n\nfig, axes = plt.subplots(9,9, figsize = (14,14))\naxes = axes.flatten()\n\n\nfor i,ax in enumerate(axes):\n    x = X_train_full[:,i]\n    y = y_train_full\n    ax.scatter(x,y, alpha= 0.5, s= 20)\n    ax.set_title(f'Feature {i+1}')\n        \n    # Set the x and y axis labels\n    ax.set_xlabel(f'Feature {i+1}')\n    ax.set_ylabel('temp')\n","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:55:59.718059Z","iopub.execute_input":"2023-05-05T11:55:59.718482Z","iopub.status.idle":"2023-05-05T11:56:15.366283Z","shell.execute_reply.started":"2023-05-05T11:55:59.718448Z","shell.execute_reply":"2023-05-05T11:56:15.365038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalize\nimport math \ndef normalize(data):\n    data_normalized = data.copy()\n    for i in range(data.shape[1]):\n        ;mean = data[:,i].mean()\n        std = np.std(data[:,i])\n        data_normalized[:,i] = (data[:,i] - mean) / std\n    return data_normalized\n\nnormalized_data = normalize(X_train_full) \nnormalized_test_data = normalize(X_test)\nfor i in range(5):\n    mean_bf = np.mean(X_train_full[:,i])\n    mean_af = np.mean(normalized_data[:,i])\n    print('Mean before standardization:=',mean_bf)\n    print('Mean after standardization:=',mean_af) \n\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:15.367618Z","iopub.execute_input":"2023-05-05T11:56:15.367997Z","iopub.status.idle":"2023-05-05T11:56:15.418371Z","shell.execute_reply.started":"2023-05-05T11:56:15.367957Z","shell.execute_reply":"2023-05-05T11:56:15.417311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which material properties may be useful for predicting superconductivity? What other observations can you make?","metadata":{}},{"cell_type":"raw","source":"Feature 36 seems to have positive correlation with the target variable","metadata":{}},{"cell_type":"markdown","source":" YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"## Task 2:  Implement your own OLS estimator\n\nWe want to use linear regression to predict the critical temperature. Implement the ordinary least squares estimator without regularization 'by hand':\n\n$w = (X^TX)^{-1}X^Ty$\n\nTo make life a bit easier, we provide a function that can be used to plot regression results. In addition it computes the mean squared error and the squared correlation between the true and predicted values. ","metadata":{}},{"cell_type":"code","source":"def plot_regression_results(y_test, y_pred, weights):\n    '''Produces three plots to analyze the results of linear regression:\n        -True vs predicted\n        -Raw residual histogram\n        -Weight histogram\n\n    Inputs:\n        y_test: (n_observations,) numpy array with true values\n        y_pred: (n_observations,) numpy array with predicted values\n        weights: (n_weights) numpy array with regression weights'''\n\n    print('MSE: ', mean_squared_error(y_test, y_pred))\n    print('r^2: ', r2_score(y_test, y_pred))\n\n    fig, ax = plt.subplots(1, 3, figsize=(9, 3))\n    # predicted vs true\n    ax[0].scatter(y_test, y_pred)\n    ax[0].set_title('True vs. Predicted')\n    ax[0].set_xlabel('True %s' % (target_clm))\n    ax[0].set_ylabel('Predicted %s' % (target_clm))\n\n    # residuals\n    error = np.squeeze(np.array(y_test)) - np.squeeze(np.array(y_pred))\n    ax[1].hist(np.array(error), bins=30)\n    ax[1].set_title('Raw residuals')\n    ax[1].set_xlabel('(true-predicted)')\n\n    # weight histogram\n    ax[2].hist(weights, bins=30)\n    ax[2].set_title('weight histogram')\n\n    plt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:15.422005Z","iopub.execute_input":"2023-05-05T11:56:15.422315Z","iopub.status.idle":"2023-05-05T11:56:15.431446Z","shell.execute_reply.started":"2023-05-05T11:56:15.422289Z","shell.execute_reply":"2023-05-05T11:56:15.430187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As an example, we here show you how to use this function with random data. ","metadata":{}},{"cell_type":"code","source":"# weights is a vector of length 82: the first value is the intercept (beta0), then 81 coefficients\nweights = np.random.randn(82)\n\n# Model predictions on the test set\ny_pred_test = np.random.randn(y_test.size)\n\nplot_regression_results(y_test, y_pred_test, weights)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:15.432764Z","iopub.execute_input":"2023-05-05T11:56:15.433097Z","iopub.status.idle":"2023-05-05T11:56:16.143933Z","shell.execute_reply.started":"2023-05-05T11:56:15.433068Z","shell.execute_reply":"2023-05-05T11:56:16.143087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Implement OLS linear regression yourself. Use `X_train` and `y_train` for estimating the weights and compute the MSE and $r^2$ from `X_test`. When you call our plotting function with the regession result, you should get mean squared error of 707.8.","metadata":{}},{"cell_type":"code","source":"def OLS_regression(X_test, X_train, y_train):\n    '''Computes OLS weights for linear regression without regularization on the training set and\n       returns weights and testset predictions.\n\n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set\n         X_train: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n\n       Outputs:\n         weights: The weight vector for the regerssion model including the offset\n         y_pred: The predictions on the TEST set\n\n       Note:\n         Both the training and the test set need to be appended manually by a columns of 1s to add\n         an offset term to the linear regression model.\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n    # adjust the data size \n    X_train = np.concatenate((np.ones((X_train.shape[0],1)),X_train),axis = 1)\n    X_test = np.concatenate((np.ones((X_test.shape[0],1)),X_test),axis = 1)\n    \n    # Compute the closed form solution\n    XtX_inv = np.linalg.inv(np.dot(X_train.T, X_train))\n    weights = np.dot(np.dot(XtX_inv, X_train.T), y_train)\n\n    # Compute predictions on the test set\n    y_pred = np.dot(X_test, weights)\n\n    # ---------------- END CODE -------------------------\n\n    return weights, y_pred","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:16.145033Z","iopub.execute_input":"2023-05-05T11:56:16.145508Z","iopub.status.idle":"2023-05-05T11:56:16.152884Z","shell.execute_reply.started":"2023-05-05T11:56:16.145479Z","shell.execute_reply":"2023-05-05T11:56:16.151549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plots of the results\n\n\nweights, y_pred = OLS_regression(normalized_test_data, normalized_data,y_train_full )\nplot_regression_results(y_test, y_pred, weights)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:16.154150Z","iopub.execute_input":"2023-05-05T11:56:16.154444Z","iopub.status.idle":"2023-05-05T11:56:16.952189Z","shell.execute_reply.started":"2023-05-05T11:56:16.154417Z","shell.execute_reply":"2023-05-05T11:56:16.950985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What do you observe? Is the linear regression model good?","metadata":{}},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"## Task 3: Compare your implementation to sklearn\n\nNow, familarize yourself with the sklearn library. In the section on linear models:\n\nhttps://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model\n\nyou will find `sklearn.linear_model.LinearRegression`, the `sklearn` implementation of the OLS estimator. Use this sklearn class to implement OLS linear regression. Again obtain estimates of the weights on `X_train` and `y_train` and compute the MSE and $r^2$ on `X_test`.\n","metadata":{}},{"cell_type":"code","source":"def sklearn_regression(X_test, X_train, y_train):\n    '''Computes OLS weights for linear regression without regularization using the sklearn library on the training set and\n       returns weights and testset predictions.\n\n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set\n         X_train: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n\n       Outputs:\n         weights: The weight vector for the regerssion model including the offset\n         y_pred: The predictions on the TEST set\n\n       Note:\n         The sklearn library automatically takes care of adding a column for the offset.\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n    model = linear_model.LinearRegression()\n    model.fit(X_train, y_train)\n    \n    weights = model.coef_[0]\n    y_pred = model.predict(X_test)\n\n    # ---------------- END CODE -------------------------\n\n    return weights, y_pred","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:16.953896Z","iopub.execute_input":"2023-05-05T11:56:16.954258Z","iopub.status.idle":"2023-05-05T11:56:16.960590Z","shell.execute_reply.started":"2023-05-05T11:56:16.954226Z","shell.execute_reply":"2023-05-05T11:56:16.959394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights, y_pred = sklearn_regression(X_test, X_train, y_train)\nplot_regression_results(y_test, y_pred, weights)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:16.962082Z","iopub.execute_input":"2023-05-05T11:56:16.962841Z","iopub.status.idle":"2023-05-05T11:56:21.898210Z","shell.execute_reply.started":"2023-05-05T11:56:16.962771Z","shell.execute_reply":"2023-05-05T11:56:21.896895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you implemented everything correctly, the MSE is again 707.8.","metadata":{}},{"cell_type":"markdown","source":"Fit the model using the larger training set, `X_train_full` and `y_train_full`, and again evaluate on `X_test`.","metadata":{}},{"cell_type":"code","source":"weights, y_pred = sklearn_regression(X_test, X_train_full, y_train_full)\nplot_regression_results(y_test, y_pred, weights)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:21.900094Z","iopub.execute_input":"2023-05-05T11:56:21.900545Z","iopub.status.idle":"2023-05-05T11:56:26.311371Z","shell.execute_reply.started":"2023-05-05T11:56:21.900494Z","shell.execute_reply":"2023-05-05T11:56:26.310289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" How does test set performance change? What else changes?","metadata":{}},{"cell_type":"markdown","source":"YOU ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"## Task 4: Regularization with ridge regression\n\nWe will now explore how a penalty term on the weights can improve the prediction quality for finite data sets. Implement the analytical solution of ridge regression \n\n$w = (X^TX + \\alpha I_D)^{-1}X^Ty$\n\n\nas a function that can take different values of $\\alpha$, the regularization strength, as an input. In the lecture, this parameter was called $\\lambda$, but this is a reserved keyword in Python.","metadata":{}},{"cell_type":"code","source":"def ridge_regression(X_test, X_train, y_train, alpha):\n    '''Computes OLS weights for regularized linear regression with regularization strength alpha\n       on the training set and returns weights and testset predictions.\n\n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set\n         `: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n         alpha: scalar, regularization strength\n\n       Outputs:\n         weights: The weight vector for the regression model including the offset\n         y_pred: The predictions on the TEST set\n\n       Note:\n         Both the training and the test set need to be appended manually by a columns of 1s to add\n         an offset term to the linear regression model.\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n    X_train  = np.concatenate((np.ones((X_train.shape[0],1)), X_train), axis = 1)\n    X_test = np.concatenate((np.ones((X_test.shape[0],1)),X_test),axis = 1)\n    \n    X_train_inv = np.linalg.inv(np.dot(X_train.T, X_train) + alpha * np.diag(np.full(X_train.shape[1], 1)))\n    weights = np.dot(X_train_inv , np.dot(X_train.T, y_train))\n    \n    y_pred = np.dot(X_test, weights)\n    \n    # ---------------- END CODE -------------------------\n\n    return weights, y_pred","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:26.312965Z","iopub.execute_input":"2023-05-05T11:56:26.313300Z","iopub.status.idle":"2023-05-05T11:56:26.320861Z","shell.execute_reply.started":"2023-05-05T11:56:26.313270Z","shell.execute_reply":"2023-05-05T11:56:26.319761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run the ridge regression on `X_train` with an alpha value of 10 and plot the obtained weights.","metadata":{}},{"cell_type":"code","source":"# Run ridge regression with alpha=10\nweights , y_pred =  ridge_regression(normalized_test_data, normalized_data,y_train_full, 10 )\n\n\n# Plot regression results\nplot_regression_results(y_test, y_pred, weights)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:26.322380Z","iopub.execute_input":"2023-05-05T11:56:26.322836Z","iopub.status.idle":"2023-05-05T11:56:27.028861Z","shell.execute_reply.started":"2023-05-05T11:56:26.322786Z","shell.execute_reply":"2023-05-05T11:56:27.027733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now test a range of log-spaced $\\alpha$s (~10-20), which cover several orders of magnitude, e.g. from 10^-7 to 10^7. \n\n* For each $\\alpha$, you will get one model with one set of weights. \n* For each model, compute the error on the test set. \n\nStore both the errors and weights of all models for later use. You can use the function `mean_squared_error` from sklearn (imported above) to compute the MSE.\n","metadata":{}},{"cell_type":"code","source":"alphas = np.logspace(-7, 7, 100)\n\n\n\n# ---------------- INSERT CODE ----------------------\narray_error = []\narray_weights = []\nfor alpha in alphas:\n    weights,y_pred = ridge_regression(X_test, X_train, y_train, alpha)\n    error = mean_squared_error(y_test, y_pred)\n    array_error.append( error)\n    array_weights.append( weights)\n\narray_error = np.array(array_error)\narray_weights = np.array(array_weights)\nprint(array_error.shape)\n# ---------------- END CODE -------------------------","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:27.030517Z","iopub.execute_input":"2023-05-05T11:56:27.030995Z","iopub.status.idle":"2023-05-05T11:56:27.211390Z","shell.execute_reply.started":"2023-05-05T11:56:27.030935Z","shell.execute_reply":"2023-05-05T11:56:27.209842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Make a single plot that shows for each coefficient how it changes with $\\alpha$, i.e. one line per coefficient. Also think about which scale is appropriate for your $\\alpha$-axis. You can set this using `plt.xscale(...)`.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Plot of coefficients vs. alphas\nnum_coef = X_train.shape[1]\n\nfor i in range(num_coef) :\n    plt.plot(alphas,array_weights[:,i], label='coef_{}'.format(i+1))\n# set plot labels and scales\nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('Coefficient')\nplt.legend()\nplt.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:27.213383Z","iopub.execute_input":"2023-05-05T11:56:27.214223Z","iopub.status.idle":"2023-05-05T11:56:29.027116Z","shell.execute_reply.started":"2023-05-05T11:56:27.214164Z","shell.execute_reply":"2023-05-05T11:56:29.025895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Why are the values of the weights largest on the left? Do they all change monotonically? ","metadata":{}},{"cell_type":"markdown","source":"The values at the left are the largest as the penality is less of the ridge regression ","metadata":{}},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"Plot how the performance (i.e. the error) changes as a function of $\\alpha$.","metadata":{}},{"cell_type":"code","source":"# Plot of MSE  vs. alphas\nplt.plot(alphas,array_error,label='coef_{}'.format(i+1) )\n    \nplt.xscale('log')\nplt.xlabel('Alpha')\nplt.ylabel('MSE')\nplt.legend()\nplt.show()\n\nmin_idx = np.argmin(array_error)\nprint(alphas[min_idx])\n\n    \n\n","metadata":{"code_folding":[],"scrolled":true,"execution":{"iopub.status.busy":"2023-05-05T11:56:29.028292Z","iopub.execute_input":"2023-05-05T11:56:29.028603Z","iopub.status.idle":"2023-05-05T11:56:29.360615Z","shell.execute_reply.started":"2023-05-05T11:56:29.028575Z","shell.execute_reply":"2023-05-05T11:56:29.359412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Which value of $\\alpha$ gives the minimum MSE? Is it better than the unregularized model? Why should the curve reach ~700 on the left?","metadata":{}},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"The best $\\alpha$ in our list is $\\sim$ 215, with a MSE of $\\sim$ 400 which is significantly better than the OLS model. The overall optimal $\\alpha$ is to be found around the order $10^2-10^3$. On the left the curve reaches $\\sim$ 600 because with $\\alpha$ approaching 0 the result of the ridge regression approaches the result of the OLS regression which has a MSE of $\\sim$ 600.","metadata":{}},{"cell_type":"markdown","source":"Now implement the same model using sklearn. Use the `linear_model.Ridge` object to do so.\n","metadata":{}},{"cell_type":"code","source":"def ridge_regression_sklearn(X_test, X_train, y_train, alpha):\n    '''Computes OLS weights for regularized linear regression with regularization strength alpha using the sklearn\n       library on the training set and returns weights and testset predictions.\n\n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set\n         X_train: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n         alpha: scalar, regularization strength\n\n       Outputs:\n         weights: The weight vector for the regerssion model including the offset\n         y_pred: The predictions on the TEST set\n\n       Note:\n         The sklearn library automatically takes care of adding a column for the offset.\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n    \n    reg = linear_model.Ridge(alpha=alpha).fit(X_train,y_train)\n    y_pred = reg.predict(X_test)\n    weights = reg.coef_\n\n    # ---------------- END CODE -------------------------\n\n    return weights, y_pred","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:29.362123Z","iopub.execute_input":"2023-05-05T11:56:29.362487Z","iopub.status.idle":"2023-05-05T11:56:29.369133Z","shell.execute_reply.started":"2023-05-05T11:56:29.362456Z","shell.execute_reply":"2023-05-05T11:56:29.368089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This time, only plot how the performance changes as a function of $\\alpha$. ","metadata":{}},{"cell_type":"code","source":"# Plot of MSE  vs. alphas\nalphas = np.logspace(-7,7,20)\nerr_arr = []\nfor a in alphas:\n    weights, y_pred = ridge_regression_sklearn(X_test, X_train, y_train,a)\n    mse = mean_squared_error(y_test,y_pred)\n    err_arr.append(mse)\n#print(err_arr)\nplt.xscale(\"log\")\nplt.plot(alphas,err_arr)\nplt.show()\n\nindex_min = 0\nfor i in range(1,len(alphas)):\n    if err_arr[i]<err_arr[index_min]:\n        index_min = i\nprint(alphas[index_min])","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:29.370582Z","iopub.execute_input":"2023-05-05T11:56:29.371201Z","iopub.status.idle":"2023-05-05T11:56:29.867003Z","shell.execute_reply.started":"2023-05-05T11:56:29.371151Z","shell.execute_reply":"2023-05-05T11:56:29.864843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: Don't worry if the curve is not exactly identical to the one you got above. The loss function we wrote down in the lecture  has $\\alpha$ defined a bit differently compared to sklearn. However, qualitatively it should look the same.","metadata":{}},{"cell_type":"markdown","source":"## Task 5: Cross-validation\n\nUntil now, we always estimated the error on the test set directly. However, we typically do not want to tune hyperparameters of our inference algorithms like $\\alpha$ on the test set, as this may lead to overfitting. Therefore, we tune them on the training set using cross-validation. As discussed in the lecture, the training data is here split in `n_folds`-ways, where each of the folds serves as a held-out dataset in turn and the model is always trained on the remaining data. Implement a function that performs cross-validation for the ridge regression parameter $\\alpha$. You can reuse functions written above.","metadata":{}},{"cell_type":"code","source":"def ridgeCV(X, y, n_folds, alphas):\n    '''Runs a n_fold-crossvalidation over the ridge regression parameter alpha.\n       The function should train the linear regression model for each fold on all values of alpha.\n\n      Inputs:\n        X: (n_obs, n_features) numpy array - predictor\n        y: (n_obs,) numpy array - target\n        n_folds: integer - number of CV folds\n        alphas: (n_parameters,) - regularization strength parameters to CV over\n\n      Outputs:\n        cv_results_mse: (n_folds, len(alphas)) numpy array, MSE for each cross-validation fold\n\n      Note:\n        Fix the seed for reproducibility.\n    '''\n\n    cv_results_mse = np.zeros((n_folds, len(alphas)))\n    np.random.seed(seed=2)\n\n    # ---------------- INSERT CODE ----------------------\n    n_points,n_params = X.shape\n    Arr = np.concatenate((X,y),axis = 1)\n    np.random.shuffle(Arr)\n    X_r = Arr[:,:n_params]\n    y_r = Arr[:,n_params:]\n    \n    splits = [0]\n    for i in range(n_folds):\n        if (i < n_points % n_folds):\n            splits.append(splits[i]+(int (n_points/n_folds))+1)\n        else:\n            splits.append(splits[i]+(int (n_points/n_folds)))\n    for j in range(len(alphas)):\n        for i in range(n_folds):\n            X_train = np.concatenate((X_r[:splits[i]],X_r[splits[i+1]:]),axis = 0)\n            y_train = np.concatenate((y_r[:splits[i]],y_r[splits[i+1]:]),axis = 0)\n            X_test = X_r[splits[i]:splits[i+1]]\n            y_test = y_r[splits[i]:splits[i+1]]\n            weights, y_pred = ridge_regression(X_test, X_train, y_train,alphas[j])\n            mse = mean_squared_error(y_test,y_pred)\n            cv_results_mse[i][j] = mse\n\n\n    # ---------------- END CODE -------------------------\n\n    return cv_results_mse","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:29.868368Z","iopub.execute_input":"2023-05-05T11:56:29.868712Z","iopub.status.idle":"2023-05-05T11:56:29.879971Z","shell.execute_reply.started":"2023-05-05T11:56:29.868683Z","shell.execute_reply":"2023-05-05T11:56:29.878678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we run 10-fold cross-validation using the training data of a range of $\\alpha$s.","metadata":{}},{"cell_type":"code","source":"alphas = np.logspace(-7, 7, 100)\nmse_cv = ridgeCV(X_train, y_train, n_folds=10, alphas=alphas)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:29.881470Z","iopub.execute_input":"2023-05-05T11:56:29.881790Z","iopub.status.idle":"2023-05-05T11:56:31.491005Z","shell.execute_reply.started":"2023-05-05T11:56:29.881762Z","shell.execute_reply":"2023-05-05T11:56:31.489384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We plot the MSE trace for each fold separately:","metadata":{}},{"cell_type":"code","source":"plt.plot(alphas, mse_cv.T, '.-')\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:31.493189Z","iopub.execute_input":"2023-05-05T11:56:31.494044Z","iopub.status.idle":"2023-05-05T11:56:31.982662Z","shell.execute_reply.started":"2023-05-05T11:56:31.493986Z","shell.execute_reply":"2023-05-05T11:56:31.981601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nplt.plot(alphas, np.mean(mse_cv, axis=0), '.-')\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()\nmse_mean = np.mean(mse_cv,axis = 0)\nind_min = 0\nfor i in range(len(mse_mean)):\n    if mse_mean[i] < mse_mean[ind_min]:\n        ind_min = i\nprint(\"optimal alpha: \",alphas[ind_min])\nprint(\"min MSE: \",mse_mean[ind_min])","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:31.984657Z","iopub.execute_input":"2023-05-05T11:56:31.985123Z","iopub.status.idle":"2023-05-05T11:56:32.427479Z","shell.execute_reply.started":"2023-05-05T11:56:31.985083Z","shell.execute_reply":"2023-05-05T11:56:32.426387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also plot the average across folds:","metadata":{}},{"cell_type":"markdown","source":"What is the optimal $\\alpha$? Is it similar to the one found on the test set? Do the cross-validation MSE and the test-set MSE match well or differ strongly?","metadata":{}},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"The optimal $\\alpha$ is $\\sim$ 2100 which is one order higher than the optimal $\\alpha$ for the test set. The cv MSE and test-set MSE match pretty well being $\\sim$ 371 and $\\sim$ 379","metadata":{}},{"cell_type":"markdown","source":"We will now run cross-validation on the full training data. This will take a moment, depending on the speed of your computer. Afterwards, we will again plot the mean CV curves for the full data set (blue) and the small data set (orange).","metadata":{}},{"cell_type":"code","source":"alphas = np.logspace(-7, 7, 100)\nmse_cv_full = ridgeCV(X_train_full, y_train_full, n_folds=10, alphas=alphas)","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:32.432035Z","iopub.execute_input":"2023-05-05T11:56:32.432397Z","iopub.status.idle":"2023-05-05T11:56:59.368816Z","shell.execute_reply.started":"2023-05-05T11:56:32.432365Z","shell.execute_reply":"2023-05-05T11:56:59.367289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nplt.plot(alphas, np.mean(mse_cv_full, axis=0), '.-')\nplt.plot(alphas, np.mean(mse_cv, axis=0), '.-')\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:59.370778Z","iopub.execute_input":"2023-05-05T11:56:59.371609Z","iopub.status.idle":"2023-05-05T11:56:59.714896Z","shell.execute_reply.started":"2023-05-05T11:56:59.371555Z","shell.execute_reply":"2023-05-05T11:56:59.713844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We zoom in on the blue curve to the very left:","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nplt.plot(alphas, np.mean(mse_cv_full, axis=0), '.-')\nplt.xscale('log')\nminValue = np.min(np.mean(mse_cv_full, axis=0))\nplt.ylim([minValue-.01, minValue+.02])\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-05-05T11:56:59.716681Z","iopub.execute_input":"2023-05-05T11:56:59.717492Z","iopub.status.idle":"2023-05-05T11:57:00.008950Z","shell.execute_reply.started":"2023-05-05T11:56:59.717448Z","shell.execute_reply":"2023-05-05T11:57:00.007631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Why does the CV curve on the full data set look so different? What is the optimal value of $\\alpha$ and why is it so much smaller than on the small training set?","metadata":{}},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}}]}