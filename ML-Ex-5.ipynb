{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"nteract":{"version":"0.24.0"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6029779,"sourceType":"datasetVersion","datasetId":3449778}],"dockerImageVersionId":30513,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Practical 5 - Dimensionality Reduction with PCA and t-SNE\n\nNames: Pratik Dhameliya,Injamam Ul Karim,Hasan Marwan Mahmood   \nSummer Term 2023   \nDue Date: Tuesday, June 27, 2pm","metadata":{}},{"cell_type":"markdown","source":"In this practical we will implement dimensionality reduction with PCA, followed by t-SNE. Following is going to be the outline:\n\n 1. Imports\n 2. Loading data and related transformations \n 3. PCA without sklearn \n 4. PCA with sklearn and comparison with 3. \n 5. t-SNE with sklearn \n\nYou will be plotting relevant results for viusalization and understanding as you go through the objectives. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(context='talk',style='white',palette='colorblind')\nimport pickle\nimport matplotlib\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport scipy as sp","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:17.923779Z","iopub.execute_input":"2023-06-30T11:23:17.924296Z","iopub.status.idle":"2023-06-30T11:23:19.282069Z","shell.execute_reply.started":"2023-06-30T11:23:17.924251Z","shell.execute_reply":"2023-06-30T11:23:19.280758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 0: Load and normalize count data","metadata":{}},{"cell_type":"markdown","source":"This practical uses the data set from https://www.nature.com/articles/s41586-018-0654-5. This is single cell transcriptomics data from ~25,000 cells from the cortex. \n\nFor each of these cells, the expression of several thousand genes was measured ```['counts']```. In the original study, the authors were interested in clustering the cells into types. \n\nWe made a selection of 5000 cells and the 1000 most informative genes for run time reasons. We provide you with the original cell type labels determined by the authors for comparison ```['clusters']```.\n\nThe following function will apply some preprocessing steps that are standard for transcriptomics data. We normalize the data to bring columns to comparable sizes and log-transform them as they contain huge outliers.","metadata":{}},{"cell_type":"code","source":"def lognormalize_counts(tasic_dict):\n\n    # normalize and logtransform counts\n    counts = tasic_dict['counts']\n    libsizes = counts.sum(axis=1)\n    CPM = counts / libsizes * 1e+6\n    logCPM = np.log2(CPM + 1)\n    tasic_dict['logCPM'] = np.array(logCPM)\n\n    return tasic_dict","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:19.284542Z","iopub.execute_input":"2023-06-30T11:23:19.284912Z","iopub.status.idle":"2023-06-30T11:23:19.291327Z","shell.execute_reply.started":"2023-06-30T11:23:19.284882Z","shell.execute_reply":"2023-06-30T11:23:19.290167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tasic_1k = lognormalize_counts(pickle.load(open('/kaggle/input/tasicdata/tasic_subset_1kselected.pickle', 'rb')))","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:19.292811Z","iopub.execute_input":"2023-06-30T11:23:19.293153Z","iopub.status.idle":"2023-06-30T11:23:19.701664Z","shell.execute_reply.started":"2023-06-30T11:23:19.293124Z","shell.execute_reply":"2023-06-30T11:23:19.700499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Have a look at ```['counts']```, ```['logPCM']``` and ```['clusters']``` to get a better understanding of the data. Plot a histogram of the cell type labels provided by  ```['clusters']```.","metadata":{}},{"cell_type":"code","source":"# Explore data\n# Explore 'counts', 'logPCM', and 'clusters' data\ncounts_data = tasic_1k['counts']\nlogPCM_data = tasic_1k['logCPM']\nclusters_data = tasic_1k['clusters']\nimport scipy.sparse\nvariable = scipy.sparse.csc_matrix(counts_data)\ndata_type = type(variable).__name__\nprint(\"'counts':\", variable)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:19.703204Z","iopub.execute_input":"2023-06-30T11:23:19.703764Z","iopub.status.idle":"2023-06-30T11:23:19.729909Z","shell.execute_reply.started":"2023-06-30T11:23:19.703725Z","shell.execute_reply":"2023-06-30T11:23:19.728716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Explore data\nprint('LogCPM values:', tasic_1k['logCPM'][:2, :4])\nprint('LogCPM shape', tasic_1k['logCPM'].shape)\nprint('counts values:', tasic_1k['counts'][:5, :8])\nprint('counts shape', tasic_1k['counts'].shape)\nprint('clusters values:', tasic_1k['clusters'][:5])\nprint('clusters shape:', tasic_1k['clusters'].shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:19.733655Z","iopub.execute_input":"2023-06-30T11:23:19.734013Z","iopub.status.idle":"2023-06-30T11:23:19.744677Z","shell.execute_reply.started":"2023-06-30T11:23:19.733984Z","shell.execute_reply":"2023-06-30T11:23:19.743549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting\nimport matplotlib.pyplot as plt\n\nplt.hist(clusters_data, bins=10)\nplt.xlabel('Cell Type')\nplt.ylabel('Frequency')\nplt.title('Histogram of Cell Type Labels')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:19.746042Z","iopub.execute_input":"2023-06-30T11:23:19.746770Z","iopub.status.idle":"2023-06-30T11:23:20.147157Z","shell.execute_reply.started":"2023-06-30T11:23:19.746735Z","shell.execute_reply":"2023-06-30T11:23:20.146037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Task 1: Linear dimensionality reduction with PCA\n\nIn this task, you will use Principal Component Analysis (PCA) to reduce the dimensionality of the dataset.\n\nFirst, implement PCA \"by hand\". You can use eigenvalue/singular value decomposition from numpy/scipy but no `sklearn`-functions. Write a function that computes all possible principal components and returns them along with the fraction of variance they explain.","metadata":{}},{"cell_type":"code","source":"def PCA_manual(data):\n    '''\n    Function that performs PCA on the input data\n\n    input: (cells, genes)-shaped array of log transformed cell counts\n    output:\n        fraction_variance_explained: (genes,)-shaped array with the fraction of variance explained by the individual PCs\n        principal_components: (genes, genes)-shaped array containing the principal components as columns\n    '''\n    ### NOTE: Make sure the function returns the PCs sorted by the fraction of variance explained! ###\n    ###       (First column of principal_components should hold the PC with the highest variance   ###\n    ###       explained -- fraction_variance_explained should also be sorted accordingly)          ###\n    ###       Also remember to center your columns by substracting the mean of each column before  ###\n    ###       you calculate the covariance matrix to get the principal components.                 ###\n\n    # ---------------- INSERT CODE ----------------------\n # Center the data by subtracting the mean of each column\n    centered_data = data - np.mean(data, axis=0)\n\n    # Compute the covariance matrix\n    covariance_matrix = np.cov(centered_data, rowvar=False)\n\n    # Perform eigenvalue decomposition on the covariance matrix\n    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n\n    # Sort the eigenvalues and eigenvectors in descending order of eigenvalues\n    sorted_indices = np.argsort(eigenvalues)[::-1]\n    sorted_eigenvalues = eigenvalues[sorted_indices]\n    sorted_eigenvectors = eigenvectors[:, sorted_indices]\n\n    # Compute the fraction of variance explained by each principal component\n    fraction_variance_explained = sorted_eigenvalues / np.sum(sorted_eigenvalues)\n\n    # Compute the principal components\n    #principal_components = np.dot(centered_data, sorted_eigenvectors)\n    principal_components = sorted_eigenvectors\n\n    # ---------------- END CODE -------------------------\n\n    return fraction_variance_explained, principal_components","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:20.148952Z","iopub.execute_input":"2023-06-30T11:23:20.149391Z","iopub.status.idle":"2023-06-30T11:23:20.159683Z","shell.execute_reply.started":"2023-06-30T11:23:20.149350Z","shell.execute_reply":"2023-06-30T11:23:20.158475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"var_expl, PCs = PCA_manual(tasic_1k['logCPM'])","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:20.161667Z","iopub.execute_input":"2023-06-30T11:23:20.162353Z","iopub.status.idle":"2023-06-30T11:23:21.869069Z","shell.execute_reply.started":"2023-06-30T11:23:20.162297Z","shell.execute_reply":"2023-06-30T11:23:21.867561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we explore the structure of the low-dimensional representation, we first want to know how much variance the first PCs explain: \n\n- Plot the fraction of variance explained by the `n`-th PC vs. `n`\n\n- Plot the cumulative fraction of variance explained by the first `n` PCs with largest eigenvalue vs. `n`\n\nFrom the latter plot you should be able to see how many PCs you need to keep to explain at least `x`% of the variance.\n\nHow many components do you need to keep to explain 50%, 75%, 90% and 99%, respectively? Indicate this in your plot.","metadata":{}},{"cell_type":"code","source":"n_PCs = len(var_expl)\nPC_ids = np.arange(1, n_PCs+1)\n\nplt.figure(figsize=(14, 7))\n\nplt.subplot(121)\n\n# Plot the variance explained of the n-th PC vs. n\n# ---------------- INSERT CODE ----------------------\nplt.plot(PC_ids, var_expl, marker='o')\nplt.xlabel('Principal Component')\nplt.ylabel('Fraction of Variance Explained')\nplt.title('Fraction of Variance Explained by Principal Components')\n# ---------------- END CODE -------------------------\n\nplt.subplot(122)\n\n# Plot the cumulative variance explained for the n PCs with highest variance explained vs. n\n# Indicate how many components you need to keep to explain 50%, 75%, 90% and 99% in the plot.\n\n# ---------------- INSERT CODE ----------------------\n\ncumulative_var_expl = np.cumsum(var_expl)\nplt.plot(PC_ids, cumulative_var_expl, marker='o')\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Cumulative Fraction of Variance Explained')\nplt.title('Cumulative Fraction of Variance Explained by Principal Components')\n\n# Indicate how many components you need to keep to explain 50%, 75%, 90% and 99% in the plot\nvar_expl_thresholds = [0.5, 0.75, 0.9, 0.99]\nfor threshold in var_expl_thresholds:\n    n_components = np.argmax(cumulative_var_expl >= threshold) + 1\n    plt.axvline(x=n_components, color='red', linestyle='--')\n    plt.text(n_components+1, threshold, f'{threshold*100}%')\n\nplt.tight_layout()\nplt.show()\n\n\n# ---------------- END CODE -------------------------","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:21.876225Z","iopub.execute_input":"2023-06-30T11:23:21.877108Z","iopub.status.idle":"2023-06-30T11:23:22.752849Z","shell.execute_reply.started":"2023-06-30T11:23:21.877046Z","shell.execute_reply":"2023-06-30T11:23:22.751711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(np.abs(np.cumsum(var_expl)[:10]-0.5))\nprint('Pcs Amount We need for 50%', np.abs(np.cumsum(var_expl) - 0.5).argmin()+1)\nprint('Pcs Amount We need for 75%', np.abs(np.cumsum(var_expl) - 0.75).argmin()+1)\nprint('Pcs Amount We need for 90%', np.abs(np.cumsum(var_expl) - 0.9).argmin()+1)\nprint('Pcs Amount We need for 99%', np.abs(np.cumsum(var_expl) - 0.99).argmin()+1)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:22.754553Z","iopub.execute_input":"2023-06-30T11:23:22.755196Z","iopub.status.idle":"2023-06-30T11:23:22.765543Z","shell.execute_reply.started":"2023-06-30T11:23:22.755159Z","shell.execute_reply":"2023-06-30T11:23:22.764580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"Write a function to select the `n` PCs needed to explain at least `x`% of the variance and use this function to extract as many PCs as are needed to explain 75% of the variance. ","metadata":{}},{"cell_type":"code","source":"def select_PCs(\n    variance_explained, principal_components, percent_variance=None):\n    '''Function that selects the first n principal components necessary to explain x% of the variance\n    input:\n        variance_explained: amount of variance explained by the individual PCs\n        principal_components: contains the principal components as columns\n        percent_variance: fraction of the variance, the all PCs that are kept explain\n    output:\n        variance_explained_kept: individual amount of variance explained for the remaining PCs\n        principal_components_kept: remaining principal components, shape (genes,n_PCs_kept)\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n\n    if percent_variance is not None:\n        # Compute cumulative sum of variance explained\n        cumulative_variance = np.cumsum(variance_explained)\n\n        # Find the index where cumulative variance exceeds or equals the desired percentage\n        n_PCs_kept = np.argmax(cumulative_variance >= percent_variance) + 1\n        print(f'Number of PCs kept: {n_PCs_kept}') # this must be 41\n\n        # Select the corresponding variance explained and principal components\n        variance_explained_kept = variance_explained[:n_PCs_kept]\n        principal_components_kept = principal_components[:, :n_PCs_kept]\n    else:\n        # If percent_variance is not provided, keep all principal components\n        variance_explained_kept = variance_explained\n        principal_components_kept = principal_components\n\n    \n    \n    # ---------------- END CODE -------------------------\n\n    return variance_explained_kept, principal_components_kept","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:22.766812Z","iopub.execute_input":"2023-06-30T11:23:22.767626Z","iopub.status.idle":"2023-06-30T11:23:22.777678Z","shell.execute_reply.started":"2023-06-30T11:23:22.767595Z","shell.execute_reply":"2023-06-30T11:23:22.776706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, PCs75 = select_PCs(var_expl, PCs, percent_variance=0.75)\nprint(PCs75.shape)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:22.778735Z","iopub.execute_input":"2023-06-30T11:23:22.779783Z","iopub.status.idle":"2023-06-30T11:23:22.797013Z","shell.execute_reply.started":"2023-06-30T11:23:22.779741Z","shell.execute_reply":"2023-06-30T11:23:22.795927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To compute the representation of the data in this lower dimensional representation, write a function that compute the PC scores for each cell, i.e. that projects the original data matrix on the low-dimensional subspace provided by the first `n` PCs:","metadata":{}},{"cell_type":"code","source":"def compute_PCA_scores(data, principal_components):\n    '''Function that returns the PC scores for each data point\n    input:\n        data                 --- (cells, genes)-shaped array of log transformed cell counts\n        principal_components --- contains the principal components as columns\n    output:\n        pc_scores            --- (cells, n_PCs_kept)-shaped array of PC scores\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n\n    # Center the data by subtracting the mean along each gene dimension\n    centered_data = data - np.mean(data, axis=0)\n\n    # Compute the PC scores by projecting the centered data onto the principal components\n    pc_scores = np.dot(centered_data, principal_components)\n\n    \n    # ---------------- END CODE -------------------------\n\n    return pc_scores","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:22.798186Z","iopub.execute_input":"2023-06-30T11:23:22.799148Z","iopub.status.idle":"2023-06-30T11:23:22.808211Z","shell.execute_reply.started":"2023-06-30T11:23:22.799116Z","shell.execute_reply":"2023-06-30T11:23:22.806982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tasic_1k['PCA_75'] = compute_PCA_scores(tasic_1k['logCPM'], PCs75)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:22.812159Z","iopub.execute_input":"2023-06-30T11:23:22.812696Z","iopub.status.idle":"2023-06-30T11:23:22.873589Z","shell.execute_reply.started":"2023-06-30T11:23:22.812665Z","shell.execute_reply":"2023-06-30T11:23:22.871935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualize the top 5 PCs as a pairwise scatterplot. Use one subplot for each pair of components.\n\nUse the colors provided in `tasic_1k['clusterColors']` and the cluster information in `tasic_1k['clusters']` to color each data point according to its original cluster identity.\n\nThe colors indicate the family of the cell type:\n\n- greenish colors: excitatory neurons\n- orange colors: somatostatin positive interneurons\n- pinkish colors: VIP-postive interneurons\n- reddish colors: parvalbumin positive interneurons\n- dark colors: non-neurons (glia etc)\n\nWhat do you observe?","metadata":{}},{"cell_type":"code","source":"def plot_PCs(data_transformed, color_per_datapoint):\n    '''Function that plots the scores of the 10 pairs of the top 5 PCs against each other.\n        inputs:\n            data_transformed    -- (cells, n_PCs_kept)-shaped array of PC scores\n            color_per_datapoint -- (cells,)-shaped array of color strings, one color for each cell\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n\n    n_components =  5 #data_transformed.shape[1]\n    \n    fig, axes = plt.subplots(n_components, n_components, figsize=(15, 15))\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n    \n    for i in range(n_components):\n        for j in range(n_components):\n            if  i != j:\n                ax = axes[i, j]\n                ax.scatter(data_transformed[:, i], data_transformed[:, j], c=color_per_datapoint)\n                ax.set_xlabel(f'PC {i+1}')\n                ax.set_ylabel(f'PC {j+1}')\n    \n    plt.show()    \n    # ---------------- END CODE -------------------------","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:22.876313Z","iopub.execute_input":"2023-06-30T11:23:22.877309Z","iopub.status.idle":"2023-06-30T11:23:22.891989Z","shell.execute_reply.started":"2023-06-30T11:23:22.877250Z","shell.execute_reply":"2023-06-30T11:23:22.890237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"color_per_datapoint = tasic_1k['clusterColors'][tasic_1k['clusters']]\nplot_PCs(tasic_1k['PCA_75'], color_per_datapoint)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:22.893886Z","iopub.execute_input":"2023-06-30T11:23:22.894339Z","iopub.status.idle":"2023-06-30T11:23:30.687827Z","shell.execute_reply.started":"2023-06-30T11:23:22.894291Z","shell.execute_reply":"2023-06-30T11:23:30.686973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"## Task 2: Comparison with PCA implemented by sklearn\n\nUse the PCA implementation of sklearn to check whether your PCA implementation is correct and obtain some insights into numerical precision of the algorithms underlying PCA implementations. Note that the sklearn implementation of PCA switches the dimensions of the matrix, so you will have to transpose your principal components matrix to get the same output as in the manual implementation.","metadata":{}},{"cell_type":"code","source":"def PCA_sklearn(data):\n    '''\n    Function that performs PCA on the input data, using sklearn\n\n    input: (cells, genes)-shaped array of log transformed cell counts\n    output:\n        fraction_variance_explained: (genes,)-shaped array with the fraction of variance explained by the individual PCs\n        principal_components: (genes, genes)-shaped array containing the principal components as columns\n    '''\n\n    # ---------------- INSERT CODE ----------------------\n    pca = PCA()\n    pca.fit(data)\n    fraction_variance_explained = pca.explained_variance_ratio_\n    principal_components = pca.components_.T\n    # ---------------- END CODE -------------------------\n\n    return fraction_variance_explained, principal_components","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:30.689159Z","iopub.execute_input":"2023-06-30T11:23:30.689704Z","iopub.status.idle":"2023-06-30T11:23:30.695031Z","shell.execute_reply.started":"2023-06-30T11:23:30.689673Z","shell.execute_reply":"2023-06-30T11:23:30.694288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# do sklearn-PCA on selected genes\nvar_expl_sklearn, PCs_sklearn = PCA_sklearn(tasic_1k['logCPM'])\n# select components as before\n_, PCs_sklearn75 = select_PCs(var_expl_sklearn, PCs_sklearn, 0.75)\n# get PC scores\nPCA_75_sklearn = compute_PCA_scores(tasic_1k['logCPM'], PCs_sklearn75)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:30.696247Z","iopub.execute_input":"2023-06-30T11:23:30.696756Z","iopub.status.idle":"2023-06-30T11:23:32.318097Z","shell.execute_reply.started":"2023-06-30T11:23:30.696726Z","shell.execute_reply":"2023-06-30T11:23:32.316500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To see if your manual PCA yielded the same PC weights as the sklearn PCA, we can just take the two matrices of principal components and plot their entries against each other. (Note: This again assumes they are sorted by variance explained and the order of dimensions in your weight matrix compared to the sklearn weight matrix is the same (change if necessary).)\n\nUse the following plot to compare the results to your own implementation (here plotting the weights of the first 100 PCs against each other). What do you observe?","metadata":{}},{"cell_type":"code","source":"n_evs_to_compare = 100\n\nplt.figure(figsize=(10, 5))\nplt.scatter(PCs_sklearn[:, :n_evs_to_compare].flatten(),\n            PCs[:, :n_evs_to_compare].flatten(), s=5, alpha=0.1)\nplt.plot([-.7, .7], [-.7, .7], ':', c='tab:gray', label='same sign', alpha=0.3)\nplt.plot([-.7, .7], [.7, -.7], '--', c='tab:gray', label='sign flipped', alpha=0.3)\nplt.legend()\nplt.xlabel('sklearn PCA weight')\nplt.ylabel('manual PCA weight')\nplt.title('Weights of the first %u manual PCs against the sklearn ones' % (n_evs_to_compare))\n\nsns.despine()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:23:32.320728Z","iopub.execute_input":"2023-06-30T11:23:32.323279Z","iopub.status.idle":"2023-06-30T11:23:36.136897Z","shell.execute_reply.started":"2023-06-30T11:23:32.323213Z","shell.execute_reply":"2023-06-30T11:23:36.135779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"YOUR ANSWER HERE","metadata":{}},{"cell_type":"markdown","source":"Additional reading about the sign of PCs: https://stats.stackexchange.com/questions/88880/does-the-sign-of-scores-or-of-loadings-in-pca-or-fa-have-a-meaning-may-i-revers","metadata":{}},{"cell_type":"markdown","source":"## Task 3: Nonlinear dimensionality reduction with t-SNE","metadata":{}},{"cell_type":"markdown","source":"In this task, you will use the nonlinear dimensionality reduction technique tSNE and look at visualizations of the data set. Plot the result of default t-SNE with the original cluster colors. For this and the following tasks, use the PCs explaining 75% of the variance ```PCA_75_sklearn``` you computed above.","metadata":{}},{"cell_type":"code","source":"def plot_tsne(tsne_results, clusters=tasic_1k['clusters'], labels=['']):\n    '''Plotting function for tsne results, creates one or multiple plots of tSNE-transformed data.\n       If the clustering is the original one (default), original cluster colors will be used. Otherwise,\n       colors will be a random permutation.\n\n    input:\n        tsne_results: (n, 2)-shaped array containing tSNE-transformed data or list of such arrays\n                      (output of the fit_transform function of sklearn tSNE)\n        clusters: (n,)-shaped array containing cluster labels or list of such arrays\n        labels: optional, list of titles for the subplots\n    '''\n\n    if type(tsne_results) == list:  # make sure we can do both single and multiple plots and are flexible regarding input\n        num_plots = len(tsne_results)\n    else:\n        num_plots = 1\n        tsne_results = [tsne_results]\n    if type(clusters) == list:\n        num_clusters = len(clusters)\n        num_plots = num_plots * num_clusters\n        tsne_results = tsne_results * num_clusters\n    else:\n        clusters = [clusters] * num_plots\n\n    if len(labels) == 1:\n        labels = labels * num_plots\n\n    n_clusters = len(np.unique(clusters))      # ensure a long enough color list even if we plot more than\n    n_colors = len(tasic_1k['clusterColors'])  # the original number of clusters\n    if n_clusters > n_colors:\n        n_extra_colors = n_clusters - n_colors\n        colors = np.concatenate((tasic_1k['clusterColors'], tasic_1k['clusterColors'][:n_extra_colors]))\n    else:\n        colors = tasic_1k['clusterColors']\n\n    fig, ax = plt.subplots(num_plots, 1, figsize=(10, num_plots*10))\n    if num_plots == 1:\n        if not np.all(tasic_1k['clusters'] == clusters[0]):\n            current_colors = np.random.permutation(colors)\n        else:\n            current_colors = colors\n        ax.scatter(tsne_results[0][:, 0], tsne_results[0][:, 1], s=1, color=current_colors[clusters[0]])\n        ax.set_title(labels[0])\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.set_axis_off()\n    else:\n        for i in range(num_plots):\n            if not np.all(tasic_1k['clusters'] == clusters[i]):\n                current_colors = np.random.permutation(colors)\n            else:\n                current_colors = colors\n            ax[i].scatter(tsne_results[i][:, 0], tsne_results[i][:, 1], s=1, color=current_colors[clusters[i]])\n            ax[i].set_title(labels[i])\n            ax[i].set_xticks([])\n            ax[i].set_yticks([])\n            ax[i].set_axis_off()","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2023-06-30T11:23:36.138389Z","iopub.execute_input":"2023-06-30T11:23:36.139168Z","iopub.status.idle":"2023-06-30T11:23:36.156650Z","shell.execute_reply.started":"2023-06-30T11:23:36.139135Z","shell.execute_reply":"2023-06-30T11:23:36.155452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run the following cells to set the random seed/random state, run tSNE and plot the results.","metadata":{}},{"cell_type":"code","source":"# fit TSNE\ntsne_default = TSNE(random_state=1)\ntsne_results = tsne_default.fit_transform(PCA_75_sklearn)","metadata":{"code_folding":[],"execution":{"iopub.status.busy":"2023-06-30T11:23:36.157984Z","iopub.execute_input":"2023-06-30T11:23:36.158383Z","iopub.status.idle":"2023-06-30T11:24:00.652017Z","shell.execute_reply.started":"2023-06-30T11:23:36.158355Z","shell.execute_reply":"2023-06-30T11:24:00.651065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting\n\noriginal_clusters = tasic_1k['clusters']\nplot_tsne(tsne_results, original_clusters, labels=['default t-SNE'])","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:24:00.653447Z","iopub.execute_input":"2023-06-30T11:24:00.654345Z","iopub.status.idle":"2023-06-30T11:24:01.039361Z","shell.execute_reply.started":"2023-06-30T11:24:00.654305Z","shell.execute_reply":"2023-06-30T11:24:01.038295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"t-SNE has one main parameter called perplexity, which trades of local and global structure. Its default value is 30. Run the tSNE with some other perplexity values (e.g. 5, 100), plot the results next to each other and explain what you observe. In particular, compare with the PCA plot above.","metadata":{}},{"cell_type":"code","source":"# try different perplexities\n\n# ---------------- INSERT CODE ----------------------\ntsne_5 = TSNE(random_state=1, perplexity=5.)\ntsne_results_5 = tsne_5.fit_transform(PCA_75_sklearn)\ntsne_20 = TSNE(random_state=1, perplexity=20.)\ntsne_results_20 = tsne_20.fit_transform(PCA_75_sklearn)\ntsne_70= TSNE(random_state=1, perplexity=70.)\ntsne_results_70 = tsne_70.fit_transform(PCA_75_sklearn)\ntsne_90 = TSNE(random_state=1, perplexity=90.)\ntsne_results_90 = tsne_90.fit_transform(PCA_75_sklearn)\ntsne_200 = TSNE(random_state=1, perplexity=200.)\ntsne_results_200 = tsne_200.fit_transform(PCA_75_sklearn)\ntsne_100 = TSNE(random_state=1, perplexity=100.)\ntsne_results_100 = tsne_100.fit_transform(PCA_75_sklearn)\n# ---------------- END CODE -------------------------","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:27:33.042140Z","iopub.execute_input":"2023-06-30T11:27:33.042642Z","iopub.status.idle":"2023-06-30T11:30:42.203747Z","shell.execute_reply.started":"2023-06-30T11:27:33.042603Z","shell.execute_reply":"2023-06-30T11:30:42.202640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot results\n\n# ---------------- INSERT CODE ----------------------\nplot_tsne(tsne_results_5, original_clusters, labels=['t-SNE perplexity=5'])\nplt.show()\nplot_tsne(tsne_results_20, original_clusters, labels=['t-SNE perplexity=20'])\nplt.show()\nplot_tsne(tsne_results_70, original_clusters, labels=['t-SNE perplexity=70'])\nplt.show()\nplot_tsne(tsne_results_90, original_clusters, labels=['t-SNE perplexity=90'])\nplt.show()\nplot_tsne(tsne_results_100, original_clusters, labels=['t-SNE perplexity=100'])\nplt.show()\nplot_tsne(tsne_results_200, original_clusters, labels=['t-SNE perplexity=200'])\nplt.show()\n# ---------------- END CODE -------------------------","metadata":{"execution":{"iopub.status.busy":"2023-06-30T11:31:20.506489Z","iopub.execute_input":"2023-06-30T11:31:20.506865Z","iopub.status.idle":"2023-06-30T11:31:22.408746Z","shell.execute_reply.started":"2023-06-30T11:31:20.506833Z","shell.execute_reply":"2023-06-30T11:31:22.407792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"YOUR ANSWER HERE\n\nCheck out https://distill.pub/2016/misread-tsne/. There's a nice tool that let's you play with t-SNE paramters and visualize the consequences.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}